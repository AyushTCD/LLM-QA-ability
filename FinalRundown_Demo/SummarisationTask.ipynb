{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "BarbieQA = pd.read_csv(\"BarbieQA.csv\")\n",
    "OppenheimerQA = pd.read_csv(\"OppenheimerQA.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers from Movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing storage.\n",
      "Mistral Response:  Ken initially finds the real world confusing and overwhelming. He struggles to understand his role in it and feels a strong desire for approval and validation from Barbie Margot. Ken also expresses frustration with the changes that have occurred in Barbie Land and attempts to regain control by manipulating situations to fit his ideal image of himself. However, he ultimately comes to realize that he must learn to accept and embrace who he truly is, both with and without Barbie. This journey leads him to understand that life is full of change and that being a leader means learning to adapt and grow.\n",
      "Llama2 Response: Based on the context information provided, it seems that Ken has a very different view of the real world compared to Barbie. While Barbie is initially excited to enter the real world and experience new things, Ken appears to be overwhelmed and disoriented by the change. He struggles to understand the complexities of the real world and finds himself feeling lost and unsure of who he is without his girlfriend Barbie by his side.\n",
      "\n",
      "Ken's thoughts on the real world may go something like this: \"Wow, everything is so different from how it was in Barbie Land. I can't believe how complex and confusing it all is. I miss Barbie already and feel lost without her to guide me. I don't know how to navigate this strange new world or who I am without her by my side. I just want things to go back to the way they were before, with Barbie and me together.\"\n",
      "\n",
      "Overall, Ken seems to be struggling to adapt to the real world and find his place in it without Barbie by his side.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import os.path\n",
    "\n",
    "Persist_DIR = \"./storageBarbie\"\n",
    "\n",
    "if not os.path.exists(Persist_DIR):\n",
    "    print(\"Initializing storage and loading documents.\")\n",
    "    documents = SimpleDirectoryReader(\"Data/Barbie\").load_data()\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=Persist_DIR)\n",
    "else:\n",
    "    print(\"Loading existing storage.\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=Persist_DIR)\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Setting up Mistral model\n",
    "Settings.llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "mistral_query_engine = index.as_query_engine()\n",
    "mistral_response = mistral_query_engine.query(\"What does Ken think about the real world when he first enters it?\")\n",
    "print(\"Mistral Response:\", mistral_response.response)\n",
    "\n",
    "# Setting up Llama2 model\n",
    "Settings.llm = Ollama(model=\"llama2\", request_timeout=30.0)\n",
    "llama2_query_engine = index.as_query_engine()\n",
    "llama2_response = llama2_query_engine.query(\"What does Ken think about the real world when he first enters it?\")\n",
    "print(\"Llama2 Response:\", llama2_response.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Score based on Semantic Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing storage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "model.safetensors: 100%|██████████| 90.9M/90.9M [00:12<00:00, 7.41MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for Mistral's response: 0.8183347582817078\n",
      "Similarity score for Llama2's response: 0.8330925107002258\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to persistently store the indexed data\n",
    "Persist_DIR = \"./storageOppenheimer\"\n",
    "\n",
    "# Check if the persistent storage directory exists; if not, initialize and load documents\n",
    "if not os.path.exists(Persist_DIR):\n",
    "    print(\"Initializing storage and loading documents.\")\n",
    "    # Assuming you have a directory 'Data/Barbie' with relevant documents\n",
    "    documents = SimpleDirectoryReader(\"Data/Oppenheimer\").load_data()\n",
    "    # Resolve and set the embedding model to be used for indexing\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    # Create an index from the documents and persist it\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=Persist_DIR)\n",
    "else:\n",
    "    print(\"Loading existing storage.\")\n",
    "    # Load the existing storage context and set the embedding model\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=Persist_DIR)\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# First question from the DataFrame\n",
    "question = OppenheimerQA[\"Question\"][0]\n",
    "\n",
    "# Setting up Mistral model for querying\n",
    "Settings.llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "mistral_query_engine = index.as_query_engine()\n",
    "mistral_response = mistral_query_engine.query(question)\n",
    "\n",
    "# Setting up Llama2 model for querying\n",
    "Settings.llm = Ollama(model=\"llama2\", request_timeout=50.0)\n",
    "llama2_query_engine = index.as_query_engine()\n",
    "llama2_response = llama2_query_engine.query(question)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming these are the responses from Mistral and Llama2 models\n",
    "mistral_response_text = mistral_response.response\n",
    "llama2_response_text = llama2_response.response\n",
    "\n",
    "# The reference text for the question from our earlier DataFrame\n",
    "reference_text = OppenheimerQA[\"Answer\"][0]\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the responses and the reference text to get their embeddings\n",
    "responses_embeddings = model.encode([mistral_response_text, llama2_response_text], convert_to_tensor=True)\n",
    "reference_embedding = model.encode(reference_text, convert_to_tensor=True)\n",
    "\n",
    "# Compute semantic similarity scores between the LLM responses and the reference text\n",
    "similarity_scores = util.pytorch_cos_sim(responses_embeddings, reference_embedding)\n",
    "\n",
    "# Print out the similarity scores for both responses\n",
    "print(f\"Similarity score for Mistral's response: {similarity_scores[0][0]}\")\n",
    "print(f\"Similarity score for Llama2's response: {similarity_scores[1][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to persistently store the indexed data\n",
    "Persist_DIR = \"./storageOppenheimer\"\n",
    "\n",
    "# Check if the persistent storage directory exists; if not, initialize and load documents\n",
    "if not os.path.exists(Persist_DIR):\n",
    "    print(\"Initializing storage and loading documents.\")\n",
    "    # Assuming you have a directory 'Data/Barbie' with relevant documents\n",
    "    documents = SimpleDirectoryReader(\"Data/Oppenheimer\").load_data()\n",
    "    # Resolve and set the embedding model to be used for indexing\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    # Create an index from the documents and persist it\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=Persist_DIR)\n",
    "else:\n",
    "    print(\"Loading existing storage.\")\n",
    "    # Load the existing storage context and set the embedding model\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=Persist_DIR)\n",
    "    Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "question = OppenheimerQA[\"Question\"][0]\n",
    "\n",
    "Settings.llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "mistral_query_engine = index.as_query_engine()\n",
    "mistral_response = mistral_query_engine.query(question)\n",
    "\n",
    "# Setting up Llama2 model for querying\n",
    "Settings.llm = Ollama(model=\"llama2\", request_timeout=50.0)\n",
    "llama2_query_engine = index.as_query_engine()\n",
    "llama2_response = llama2_query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissertationLLM",
   "language": "python",
   "name": "dissertationllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
